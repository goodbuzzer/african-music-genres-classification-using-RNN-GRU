{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6530b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import math\n",
    "import librosa\n",
    "import warnings\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.layers import Conv1D, BatchNormalization, GRU, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad436968",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "# Spécifiez le chemin du dossier dans lequel vous souhaitez créer le fichier\n",
    "dossier = \"C:/Users/asus/Desktop/rnn_nounamo\"\n",
    "\n",
    "# Assurez-vous que le dossier existe, sinon, créez-le\n",
    "if not os.path.exists(dossier):\n",
    "    os.makedirs(dossier)\n",
    "\n",
    "\n",
    "# Construisez le chemin complet du fichier data.json\n",
    "chemin_fichier = os.path.join(dossier, \"data_5.json\")\n",
    "\n",
    "# Écrivez les données dans le fichier JSON\n",
    "with open(chemin_fichier, 'w') as fichier_json:\n",
    "    json.dump({}, fichier_json, indent=4)\n",
    "\n",
    "print(f'Le fichier data.json a été créé dans le dossier : {dossier}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9141bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import math\n",
    "import librosa\n",
    "import warnings\n",
    "\n",
    "# Ignorer les avertissements FutureWarning spécifiques à librosa\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"librosa\")\n",
    "\n",
    "DATASET_PATH = \"C:/Users/asus/Desktop/rnn_nounamo/africa_songs\"\n",
    "JSON_PATH = \"C:/Users/asus/Desktop/rnn_nounamo/data.json\"\n",
    "SAMPLE_RATE = 22050\n",
    "TRACK_DURATION = 30 # measured in seconds\n",
    "SAMPLES_PER_TRACK = SAMPLE_RATE * TRACK_DURATION\n",
    "\n",
    "\n",
    "def save_mfcc(dataset_path, json_path, num_mfcc=13, n_fft=2048, hop_length=512, num_segments=5):\n",
    "    \"\"\"Extracts MFCCs from music dataset and saves them into a json file along witgh genre labels.\n",
    "\n",
    "        :param dataset_path (str): Path to dataset\n",
    "        :param json_path (str): Path to json file used to save MFCCs\n",
    "        :param num_mfcc (int): Number of coefficients to extract\n",
    "        :param n_fft (int): Interval we consider to apply FFT. Measured in # of samples\n",
    "        :param hop_length (int): Sliding window for FFT. Measured in # of samples\n",
    "        :param: num_segments (int): Number of segments we want to divide sample tracks into\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "    # dictionary to store mapping, labels, and MFCCs\n",
    "    data = {\n",
    "        \"mapping\": [],\n",
    "        \"labels\": [],\n",
    "        \"mfcc\": []\n",
    "    }\n",
    "\n",
    "    samples_per_segment = int(SAMPLES_PER_TRACK / num_segments)\n",
    "    num_mfcc_vectors_per_segment = math.ceil(samples_per_segment / hop_length)\n",
    "\n",
    "    # loop through all genre sub-folder\n",
    "    for i, (dirpath, dirnames, filenames) in enumerate(os.walk(dataset_path)):\n",
    "\n",
    "        # ensure we're processing a genre sub-folder level\n",
    "        if dirpath is not dataset_path:\n",
    "\n",
    "            # save genre label (i.e., sub-folder name) in the mapping\n",
    "            semantic_label = dirpath.split(\"/\")[-1]\n",
    "            data[\"mapping\"].append(semantic_label)\n",
    "            print(\"\\nProcessing: {}\".format(semantic_label))\n",
    "\n",
    "            # process all audio files in genre sub-dir\n",
    "            for f in filenames:\n",
    "\n",
    "# load audio file\n",
    "                file_path = os.path.join(dirpath, f)\n",
    "                signal, sample_rate = librosa.load(file_path, sr=SAMPLE_RATE)\n",
    "\n",
    "                # process all segments of audio file\n",
    "                for d in range(num_segments):\n",
    "\n",
    "                    # calculate start and finish sample for current segment\n",
    "                    start = samples_per_segment * d\n",
    "                    finish = start + samples_per_segment\n",
    "\n",
    "                    # extract mfcc\n",
    "                    mfcc = librosa.feature.mfcc(y=signal[start:finish],sr=sample_rate, n_mfcc=num_mfcc, n_fft=n_fft, hop_length=hop_length)\n",
    "                    mfcc = mfcc.T\n",
    "\n",
    "                    # store only mfcc feature with expected number of vectors\n",
    "                    if len(mfcc) == num_mfcc_vectors_per_segment:\n",
    "                        data[\"mfcc\"].append(mfcc.tolist())\n",
    "                        data[\"labels\"].append(i-1)\n",
    "                        print(\"{}, segment:{}\".format(file_path, d+1))\n",
    "\n",
    "    # save MFCCs to json file\n",
    "    with open(json_path, \"w\") as fp:\n",
    "        json.dump(data, fp, indent=4)\n",
    "        \n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    save_mfcc(DATASET_PATH, JSON_PATH, num_segments=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667aff4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.layers import Conv1D, BatchNormalization, GRU, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Chemin vers le fichier JSON contenant les données\n",
    "DATA_PATH = \"C:/Users/asus/Desktop/rnn_nounamo/data.json\"\n",
    "\n",
    "# Fonction pour charger les données à partir du fichier JSON\n",
    "def load_data(data_path):\n",
    "    with open(data_path, \"r\") as fp:\n",
    "        data = json.load(fp)\n",
    "\n",
    "    X = np.array(data[\"mfcc\"])\n",
    "    y = np.array(data[\"labels\"])\n",
    "    return X, y\n",
    "\n",
    "# Fonction pour afficher les courbes d'entraînement\n",
    "def plot_history(history):\n",
    "    fig, axs = plt.subplots(2)\n",
    "\n",
    "    axs[0].plot(history.history[\"accuracy\"], label=\"train accuracy\")\n",
    "    axs[0].plot(history.history[\"val_accuracy\"], label=\"validation accuracy\")\n",
    "    axs[0].set_ylabel(\"Accuracy\")\n",
    "    axs[0].legend(loc=\"lower right\")\n",
    "    axs[0].set_title(\"Accuracy Evaluation\")\n",
    "\n",
    "    axs[1].plot(history.history[\"loss\"], label=\"train loss\")\n",
    "    axs[1].plot(history.history[\"val_loss\"], label=\"validation loss\")\n",
    "    axs[1].set_ylabel(\"Loss\")\n",
    "    axs[1].set_xlabel(\"Epoch\")\n",
    "    axs[1].legend(loc=\"upper right\")\n",
    "    axs[1].set_title(\"Loss Evaluation\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# Fonction pour préparer les ensembles de données d'entraînement, de validation et de test\n",
    "def prepare_datasets(test_size, validation_size):\n",
    "    X, y = load_data(DATA_PATH)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42)\n",
    "    X_train, X_validation, y_train, y_validation = train_test_split(\n",
    "        X_train, y_train, test_size=validation_size, random_state=42\n",
    "    )\n",
    "\n",
    "    return X_train, X_validation, X_test, y_train, y_validation, y_test\n",
    "\n",
    "# Fonction pour construire le modèle\n",
    "def build_model(input_shape):\n",
    "    model = keras.Sequential()\n",
    "    \n",
    "    model.add(Conv1D(64, 5, strides=2, padding='same', activation='relu', input_shape=input_shape))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    model.add(GRU(128, return_sequences=True, kernel_regularizer=keras.regularizers.l2(0.01)))\n",
    "    model.add(GRU(128, kernel_regularizer=keras.regularizers.l2(0.01)))\n",
    "    \n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dropout(0.3))\n",
    "    \n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "    return model\n",
    "\n",
    "# Partie principale du script\n",
    "if __name__ == \"__main__\":\n",
    "    # Chargement des données et préparation des ensembles\n",
    "    X_train, X_validation, X_test, y_train, y_validation, y_test = prepare_datasets(0.25, 0.2)\n",
    "\n",
    "    # Définition de la forme d'entrée du modèle\n",
    "    input_shape = (X_train.shape[1], X_train.shape[2])\n",
    "\n",
    "    # Construction du modèle\n",
    "    model = build_model(input_shape)\n",
    "\n",
    "    # Compilation du modèle avec l'optimiseur, la fonction de perte et les métriques\n",
    "    optimizer = Adam(learning_rate=0.0001)\n",
    "    model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    # Affichage de la structure du modèle\n",
    "    model.summary()\n",
    "\n",
    "    # Définition des callbacks pour l'arrêt précoce et la sauvegarde du meilleur modèle\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "    checkpoint = ModelCheckpoint('best_model.h5', monitor='val_accuracy', save_best_only=True)\n",
    "\n",
    "    # Entraînement du modèle\n",
    "    history = model.fit(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        validation_data=(X_validation, y_validation),\n",
    "        batch_size=32,\n",
    "        epochs=100,\n",
    "        callbacks=[early_stopping, checkpoint],\n",
    "        verbose=1,\n",
    "    )\n",
    "\n",
    "    # Affichage des courbes d'entraînement\n",
    "    plot_history(history)\n",
    "\n",
    "    # Évaluation du modèle sur l'ensemble de test\n",
    "    test_loss, test_acc = model.evaluate(X_test, y_test, verbose=2)\n",
    "    print('\\nPrécision sur le test:', test_acc)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
